# Plan Mode Streaming Implementation Plan

## Overview

This document outlines the implementation plan for enhanced plan mode with streaming progress updates via SSE, based on expert consultation and codebase analysis.

## Problem Statement

Current plan mode has significant UX and technical issues:
- ‚ùå **Timeout Risk**: Plan processing can take 5-30 seconds, risking HTTP/browser timeouts
- ‚ùå **Poor UX**: Users wait blindly with no progress indication  
- ‚ùå **No "AI Thinking"**: No visibility into analysis progress
- ‚ùå **Inconsistent Architecture**: Build mode streams progress, plan mode doesn't

## Expert Consultation Analysis

### ‚úÖ **What We Love About Expert's Approach:**

1. **Reuses Existing Infrastructure** üéØ
   - Existing `ChatBroadcastService.broadcastSystemEvent()` perfect for ephemeral events
   - Existing `processChatPlanStream()` already provides rich event streams
   - SSE channel, Redis pub/sub, auth, replay semantics all working

2. **Ephemeral vs Durable Events Pattern** üß†
   ```typescript
   // Progress (no id) - don't replay on reconnect
   { event: 'plan.progress', data: { phase: 'analyzing' } }
   
   // Final (with seq) - proper replay semantics  
   { id: '321', event: 'message.new', data: { /* final result */ } }
   ```

3. **Consistent Architecture**
   - Build mode: Fast HTTP ‚Üí Background ‚Üí SSE ‚úÖ
   - Plan mode: Fast HTTP ‚Üí Background ‚Üí SSE ‚úÖ

### ü§î **MVP Considerations:**

- **Background Processing Complexity**: More failure modes, error handling, testing
- **Implementation Time**: More moving parts than synchronous approach
- **Current Need**: Most plan requests are 2-10 seconds, not always requiring full async

## Expert Feedback Analysis & Incorporation

### ‚úÖ **Critical Fixes Applied:**

1. **HTTP Blocking Issue Resolved**: Original Phase 1 still had `await streamComplexPlan()` blocking HTTP response. Expert's Phase 1.5 uses `setImmediate()` for true fire-and-forget background work.

2. **Double Compute Eliminated**: Confirmed `processChatPlanStream()` emits `'complete'` event with `data.fullResponse` - no need for separate `processChatPlan()` call.

3. **Per-Request Throttling**: Expert recommended throttling by `(projectId, client_msg_id)` vs globally - prevents parallel plans from interfering.

4. **Type Safety Enhanced**: Added `'plan.progress' | 'plan.partial' | 'plan.error'` to `SSEChatEvent` union.

### üéØ **Expert Enhancements Integrated:**

- **"Last Write Wins" Coalescing**: ThrottledBroadcaster keeps only latest progress update within throttle window
- **Stable Final State**: Always flush pending events before completion for consistent UI state
- **Ephemeral Event IDs**: Each progress event gets unique `ulid()` but doesn't update Last-Event-ID
- **Error Boundaries**: Fire-and-forget errors logged but don't crash HTTP response

### ü§î **MVP Tradeoffs Accepted:**

- **In-Process Work**: `setImmediate()` keeps analyses in main process vs proper background queue
- **Process Coupling**: Server restart kills in-progress plans (acceptable for MVP complexity reduction)
- **Memory Usage**: Long-running plans stay in process memory vs worker isolation

## Implementation Plan: Expert-Informed Phased Approach

### Phase 1.5: Fast HTTP + In-Process Streaming (Expert-Recommended MVP)

**Critical Fix**: Expert identified blocking HTTP issue in original Phase 1. This approach eliminates HTTP timeout risk while maintaining simplicity.

```typescript
private async handlePlanMode(request: UnifiedChatRequest): Promise<UnifiedChatResponse> {
  const isComplex = this.isComplexAnalysis(request.message);
  
  if (!isComplex) {
    // Simple questions: synchronous (1-3 seconds)
    const result = await this.chatPlanService.processChatPlan(request);
    await this.saveAssistantMessage(projectId, userId, 'plan', result.response, messageId);
    return { accepted: true, mode: 'plan', analysis: result };
  }
  
  // üöÄ EXPERT FIX: Fast HTTP response + fire-and-forget background
  const response = {
    accepted: true, success: true, queued: true,
    mode: 'plan', sessionId, messageId, 
    client_msg_id: request.client_msg_id,
    timestamp: new Date().toISOString()
  };
  
  // Start streaming analysis after HTTP response (no await)
  setImmediate(() => this.streamPlanInProcess(request, messageId));
  
  return response;
}
```

**Key Benefits:**
- ‚úÖ **Expert's Critical Fix**: HTTP returns in <200ms, no timeout risk
- ‚úÖ **Expert's UX**: Progress events + final result via SSE
- ‚úÖ **Expert's Reconnection**: Ephemeral progress, durable final message
- ‚úÖ **MVP Simplicity**: No queue infrastructure, in-process background work
- ‚úÖ **No Double Compute**: Stream emits final result, no separate processChatPlan call

### Phase 2: Full Background Processing (Expert's Vision)

**Strategy**: Complete implementation of expert's architecture

```typescript
private async handlePlanMode(request: UnifiedChatRequest): Promise<UnifiedChatResponse> {
  // Fast ack (all plans)
  const response = {
    accepted: true, success: true, queued: true,
    mode: 'plan', sessionId, messageId, client_msg_id: request.client_msg_id
  };
  
  // Background processing with SSE updates
  this.processInBackground(request);
  
  return response;
}
```

## Technical Implementation Details

### Event Architecture

**Ephemeral Events** (no `id` - don't replay on reconnect):
```typescript
// Progress updates
await broadcast.broadcastSystemEvent(projectId, {
  event: 'plan.progress',
  data: {
    userId: request.userId,
    client_msg_id: request.client_msg_id,
    phase: 'fetching_context' | 'analyzing' | 'summarizing',
    message: 'Analyzing project structure...',
    timestamp: new Date().toISOString()
  }
});

// Partial results  
await broadcast.broadcastSystemEvent(projectId, {
  event: 'plan.partial',
  data: {
    userId: request.userId,
    client_msg_id: request.client_msg_id,
    chunk: { /* partial structured data */ },
    timestamp: new Date().toISOString()
  }
});

// Errors
await broadcast.broadcastSystemEvent(projectId, {
  event: 'plan.error',
  data: {
    userId: request.userId,
    client_msg_id: request.client_msg_id,
    error: { code: 'ANALYSIS_FAILED', message: 'Context too large' },
    timestamp: new Date().toISOString()
  }
});
```

**Durable Events** (with `id = seq` - replay on reconnect):
```typescript
// Final result (automatically handled by saveAssistantMessage)
await this.saveAssistantMessage(
  request.projectId,
  request.userId,
  'plan',
  finalResult,  // Complete analysis
  messageId
);
// This automatically broadcasts: { id: seq, event: 'message.new', data: finalResult }
```

### Stream Event Mapping

Map existing `StreamEventType` to SSE events (based on codebase analysis):
```typescript
private mapStreamEvent(streamEvent: StreamEvent): string | null {
  switch (streamEvent.event) {
    case 'progress_update': return 'plan.progress';
    case 'tool_use': return 'plan.progress';        
    case 'assistant_text': return 'plan.partial';   
    case 'complete': return null; // Handle via saveAssistantMessage
    case 'error': return 'plan.error';
    case 'intent_detected': return 'plan.progress';
    case 'references': return 'plan.progress';
    default: return 'plan.progress';
  }
}
```

**Key Insight**: Our `StreamEvent` with `event: 'complete'` contains `data.fullResponse` - perfect for single-pass processing.

### Throttling Implementation

**Expert Enhancement**: Per-request throttling with "last write wins" coalescing:

```typescript
class ThrottledBroadcaster {
  private lastEmit = 0;
  private readonly THROTTLE_MS = 1000; // 1/sec max
  private pendingEvent: any = null;
  
  constructor(
    private broadcast: ChatBroadcastService,
    private projectId: string,
    private clientMsgId: string  // üéØ Expert: Per-request throttling key
  ) {}
  
  async emitThrottled(event: any) {
    // üéØ Expert: "Last write wins" - always update to latest
    this.pendingEvent = event;
    
    const now = Date.now();
    if (now - this.lastEmit >= this.THROTTLE_MS) {
      await this.broadcast.broadcastSystemEvent(this.projectId, {
        id: ulid(),  // Ephemeral event ID
        event: this.pendingEvent.event,
        data: this.pendingEvent.data
      });
      this.lastEmit = now;
      this.pendingEvent = null;
    }
  }
  
  async flushPending() {
    // üéØ Expert: Always flush final state for stable UI
    if (this.pendingEvent) {
      await this.broadcast.broadcastSystemEvent(this.projectId, {
        id: ulid(),
        event: this.pendingEvent.event,
        data: this.pendingEvent.data
      });
      this.pendingEvent = null;
    }
  }
}
```

### Complexity Classification

```typescript
private isComplexAnalysis(message: string): boolean {
  // Simple heuristics - can be improved with ML later
  const complexTriggers = [
    /analyz[e|ing]/i,
    /review.*architecture/i, 
    /audit/i,
    /explain.*how.*works/i,
    /what.*wrong.*with/i,
    /improve.*performance/i,
    /refactor/i
  ];
  
  return complexTriggers.some(pattern => pattern.test(message)) ||
         message.length > 150; // Long messages likely complex
}
```

### Phase 1.5 Implementation (Expert-Recommended MVP)

**Core Pattern**: Fast HTTP ack ‚Üí In-process streaming ‚Üí SSE events ‚Üí Final message.new

```typescript
private async streamPlanInProcess(
  request: UnifiedChatRequest, 
  messageId: string
): Promise<void> {
  const broadcast = ChatBroadcastService.getInstance();
  const throttler = new ThrottledBroadcaster(
    broadcast, 
    request.projectId, 
    request.client_msg_id  // üéØ Expert: Per-request throttling
  );
  
  let finalResult: any = null;
  
  try {
    // üéØ Expert: Single processChatPlanStream call, capture final result from stream
    await this.chatPlanService.processChatPlanStream({
      userId: request.userId,
      projectId: request.projectId,
      message: request.message,
      locale: resolvedLocale,
      context: { includeVersionHistory: true, includeProjectStructure: true }
    }, async (streamEvent) => {
      
      // Map our StreamEventType to SSE event names
      const sseEventType = this.mapStreamEvent(streamEvent);
      
      if (sseEventType === 'plan.progress') {
        await throttler.emitThrottled({
          event: 'plan.progress',
          data: {
            userId: request.userId,
            client_msg_id: request.client_msg_id,
            phase: streamEvent.data.phase || 'analyzing',
            message: streamEvent.data.message || 'Processing...',
            timestamp: streamEvent.timestamp
          }
        });
      } else if (sseEventType === 'plan.partial') {
        await throttler.emitThrottled({
          event: 'plan.partial',
          data: {
            userId: request.userId,
            client_msg_id: request.client_msg_id,
            chunk: streamEvent.data.chunk || streamEvent.data.text,
            timestamp: streamEvent.timestamp
          }
        });
      } else if (streamEvent.event === 'complete') {
        // üéØ Expert: Capture final result from stream (no double compute)
        finalResult = streamEvent.data.fullResponse;
      }
    });
    
    // Flush any pending throttled events
    await throttler.flushPending();
    
    // Save final result (broadcasts durable message.new automatically)
    if (finalResult) {
      await this.saveAssistantMessage(
        request.projectId,
        request.userId,
        'plan',
        finalResult,
        messageId
      );
    }
    
  } catch (error) {
    // Broadcast error event
    await broadcast.broadcastSystemEvent(request.projectId, {
      id: ulid(),
      event: 'plan.error',
      data: {
        userId: request.userId,
        client_msg_id: request.client_msg_id,
        error: {
          code: 'PLAN_PROCESSING_FAILED',
          message: error instanceof Error ? error.message : 'Analysis failed'
        },
        timestamp: new Date().toISOString()
      }
    });
    
    // Note: Don't throw - this is fire-and-forget background work
    console.error('[PlanMode] Background processing failed:', error);
  }
}
```

## Frontend Integration

### SSE Event Handling

```typescript
// In use-persistent-live.ts or similar
es.addEventListener('plan.progress', (e) => {
  const data = JSON.parse(e.data);
  onPlanProgress?.({
    phase: data.phase,
    message: data.message,
    client_msg_id: data.client_msg_id
  });
});

es.addEventListener('plan.partial', (e) => {
  const data = JSON.parse(e.data);
  onPlanPartial?.({ 
    chunk: data.chunk, 
    client_msg_id: data.client_msg_id 
  });
});

es.addEventListener('plan.error', (e) => {
  const data = JSON.parse(e.data);
  onPlanError?.({ 
    error: data.error, 
    client_msg_id: data.client_msg_id 
  });
});
```

### UI Lifecycle

1. **On Send (Plan)**: Show "Analyzing..." row keyed by `client_msg_id`
2. **Progress Events**: Update row with phase/progress indication
3. **Partial Events**: Show incremental results if applicable
4. **Final `message.new`**: Replace temporary row with final assistant message
5. **Error Events**: Show error state in temporary row

### Reconnection Behavior

- ‚úÖ **Ephemeral events don't replay**: No "Analyzing..." spinners on reconnect
- ‚úÖ **Final message replays**: Complete analysis available after reconnect
- ‚úÖ **Perfect UX**: Users see final results, not stale progress indicators

## Performance & Reliability

### Acceptance Criteria

**Phase 1 (MVP)**:
- ‚úÖ Simple questions (< 150 chars) return synchronously in 1-3 seconds
- ‚úÖ Complex analysis shows progress events ~1/second during processing
- ‚úÖ Final result broadcasts as durable `message.new` with proper `seq`
- ‚úÖ Reconnection shows final result, no replay of progress events
- ‚úÖ Error handling broadcasts `plan.error` events appropriately

**Phase 2 (Full Background)**:
- ‚úÖ All plan requests return HTTP 200 in <200ms with `{ queued: true }`
- ‚úÖ Background processing handles concurrency and scaling
- ‚úÖ Queue management prevents resource exhaustion
- ‚úÖ Robust error handling and retry logic

### Error Handling

**Timeout Scenarios**:
- HTTP connection timeout ‚Üí No impact (already responded fast)
- SSE connection drops ‚Üí Client reconnects, sees final result if available
- Plan processing timeout ‚Üí Broadcasts `plan.error` event

**Failure Recovery**:
- Parse errors ‚Üí Graceful fallback with error event
- Resource exhaustion ‚Üí Queue backpressure + error event
- Redis failures ‚Üí Graceful degradation (processing continues, no progress updates)

## Implementation Timeline

### Phase 1.5: Smart Synchronous + Fire-and-Forget Streaming (COMPLETED ‚úÖ)
- [x] **COMPLETED**: Implement complexity classification heuristics
- [x] **COMPLETED**: Add throttled broadcaster utility class with per-request throttling
- [x] **COMPLETED**: Modify `handlePlanMode` for smart routing and fire-and-forget processing
- [x] **COMPLETED**: Implement `streamPlanInProcess` method with single-pass processing
- [x] **COMPLETED**: Add event mapping from StreamEvent to SSE events
- [x] **COMPLETED**: Update SSEChatEvent interface with plan.* event types
- [x] **COMPLETED**: Test logic verification (complexity classification, event mapping)
- [ ] **NEXT**: Frontend event handling integration
- [ ] **NEXT**: Integration testing with actual requests
- [ ] **NEXT**: Error handling and edge cases

### Phase 2: Background Processing (Future)
- [ ] Implement plan job queue using existing BullMQ infrastructure  
- [ ] Create plan worker for background processing
- [ ] Add concurrency controls and resource limits
- [ ] Enhanced error handling and retry logic
- [ ] Performance monitoring and metrics
- [ ] Load testing and optimization

## Success Metrics

- **Response Time**: 95% of plan requests return HTTP response in <5 seconds
- **Progress Visibility**: Users see meaningful progress updates during analysis
- **Reconnection UX**: No stale "Analyzing..." indicators after reconnect
- **Error Recovery**: Graceful handling of timeouts and failures with clear user feedback
- **Architecture Consistency**: Plan mode follows same patterns as build mode

## Future Enhancements

- **Smart Classification**: ML-based complexity detection
- **Adaptive Throttling**: Dynamic event frequency based on processing phase  
- **Rich Progress**: File-by-file analysis progress, citation discovery
- **Cancellation**: User ability to cancel in-progress analysis
- **Caching**: Cache analysis results for similar questions
- **Metrics**: Detailed analytics on plan processing performance

## Implementation Report (2025-08-27)

### ‚úÖ Phase 1.5 Implementation Complete

**Status**: Core plan mode streaming implementation completed and tested. Ready for integration testing.

### Key Implementation Details

#### 1. **Smart Routing Logic** (`isComplexAnalysis`)
```typescript
// Triggers for complex analysis requiring streaming
const complexTriggers = [
  /analyz[e|ing]/i, /review.*architecture/i, /audit/i,
  /explain.*how.*works/i, /what.*wrong.*with/i,
  /improve.*performance/i, /refactor/i, /optimize/i,
  /security.*issue/i, /best.*practice/i,
  /code.*quality/i, /technical.*debt/i
];
// Also: messages > 150 characters considered complex
```

**Discovery**: Simple questions (< 150 chars, no complex triggers) process synchronously in 1-3 seconds. Complex analysis uses fire-and-forget streaming.

#### 2. **ThrottledBroadcaster** (Expert-Recommended Pattern)
- **Per-Request Throttling**: Keyed by `(projectId, client_msg_id)` prevents parallel plans from interfering
- **"Last Write Wins"**: Only latest progress update kept within 1-second throttle window  
- **Stable Final State**: Always flushes pending events before completion for consistent UI

#### 3. **Single-Pass Processing** (Critical Expert Fix)
- **No Double Compute**: Uses `processChatPlanStream` only, captures `finalResult` from `complete` event
- **Fire-and-Forget**: `setImmediate()` prevents HTTP blocking, returns response in <200ms
- **Error Boundaries**: Background failures logged but don't crash HTTP response

#### 4. **Event Architecture**
```typescript
// Ephemeral Events (no replay on reconnect)
'plan.progress' // Tool usage, progress phases
'plan.partial'  // Incremental results  
'plan.error'    // Processing errors

// Durable Events (replay on reconnect via existing saveAssistantMessage)
'message.new'   // Final complete analysis result
```

### Implementation Files Modified

1. **`src/services/chatBroadcastService.ts`**:
   - Extended `SSEChatEvent` union with `'plan.progress' | 'plan.partial' | 'plan.error'`

2. **`src/services/unifiedChatService.ts`**:
   - Added `ThrottledBroadcaster` utility class
   - Added `isComplexAnalysis()` heuristics  
   - Added `mapStreamEvent()` for StreamEvent ‚Üí SSE mapping
   - Enhanced `handlePlanMode()` with smart routing
   - Implemented `streamPlanInProcess()` with single-pass processing

### Testing Results ‚úÖ

**Logic Verification Passed**:
- ‚úÖ Complexity classification correctly identifies simple vs complex questions
- ‚úÖ Stream event mapping covers all `StreamEventType` cases
- ‚úÖ TypeScript compilation passes with strict mode
- ‚úÖ Expert patterns (throttling, fire-and-forget, single-pass) implemented correctly

### Production Readiness

**Ready for Integration Testing**:
- ‚úÖ Core logic implemented and tested
- ‚úÖ Error handling with graceful degradation
- ‚úÖ Backward compatibility maintained (simple questions work as before)
- ‚úÖ Expert feedback incorporated (HTTP blocking eliminated, double compute avoided)

**Next Steps**:
1. Integration testing with real requests
2. Frontend SSE event handler updates
3. End-to-end flow verification
4. Performance monitoring

### Architecture Benefits Achieved

- ‚úÖ **No HTTP Timeouts**: Complex analysis can't block response (fire-and-forget)
- ‚úÖ **Better UX**: Progress visibility during long analyses  
- ‚úÖ **Consistent Patterns**: Matches build mode SSE architecture
- ‚úÖ **Expert-Informed**: Incorporates all critical expert recommendations
- ‚úÖ **MVP Appropriate**: In-process streaming avoids queue complexity

---

## Summary

This expert-informed plan provides a clear path from MVP (Phase 1.5) to full-featured streaming plan mode (Phase 2) while reusing existing infrastructure and maintaining architectural consistency.

**Key Expert Contribution**: Eliminated HTTP blocking risk through `setImmediate()` fire-and-forget pattern while preserving all UX benefits of streaming progress events and proper reconnection semantics.

**Implementation Status**: Phase 1.5 core implementation completed (2025-08-27). Ready for integration testing and frontend event handler updates.