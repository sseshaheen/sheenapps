The foundations look solid, but for an MVP you‚Äôre spreading focus across too many ‚Äúnice-to-haves.‚Äù I‚Äôd tighten scope around: reliability of builds, correctness of state, cost control, and observability. Everything else can wait.

What I‚Äôd change now (MVP-first, in priority order)
	1.	Events > polling (but keep it simple): use SSE first, not WebSockets.
	‚Ä¢	One /events?buildId=... SSE endpoint from the Worker; reconnect with Last-Event-ID.
	‚Ä¢	Push only canonical events from the Worker; the UI should not infer state.
	‚Ä¢	Keep polling as a fallback for 48h behind a flag.

	2.	Make builds idempotent and transactional.
	‚Ä¢	Every mutating call carries an Idempotency-Key and projectId.
	‚Ä¢	Worker holds a state machine for build_status (Postgres CHECK or trigger) to prevent illegal transitions.
	‚Ä¢	Reserve user credits before starting the build; commit or release on success/failure.

	3.	Collapse preview paths to one approach.
	‚Ä¢	Kill ‚ÄúReact Preview (experimental)‚Äù for MVP; it invites style leakage and debug time.
	‚Ä¢	Keep the iframe preview on a separate origin (preview.*) with tight sandbox + CSP.
	‚Ä¢	Add TTL/garbage collection for stale previews.

	4.	Fix Hook violations and state ownership, then freeze the pattern.
	‚Ä¢	Replace the global ‚Äúbuild state store‚Äù with:
	‚Ä¢	Server state = React Query (SSE source hydrates cache).
	‚Ä¢	UI state = a tiny Zustand store (panel open, selected section).
	‚Ä¢	Ban conditional hooks with ESLint rule + codemod the offenders.

	5.	Harden the Worker client & backpressure.
	‚Ä¢	Timeouts + jittered retries + circuit breaker per provider (half-open after backoff).
	‚Ä¢	Surface 429 and 5xx distinctly; show ‚Äúwe‚Äôll notify you‚Äù once queue depth > threshold.

	6.	Observability before performance ‚Äúpolish.‚Äù
	‚Ä¢	Structured logs (Pino) with requestId, buildId, userId, projectId.
	‚Ä¢	Minimal metrics: build_latency, queue_depth, build_success_rate, provider_error_rate, token_cost.
	‚Ä¢	Error tracking (Sentry or similar) wired to the same IDs. Dashboards later.

	7.	Tighten security around preview + secrets.
	‚Ä¢	Iframe sandbox="allow-scripts allow-same-origin" only on a cookieless subdomain; no auth cookies there.
	‚Ä¢	Strict CSP (script-src only your preview host + nonce); COOP/COEP on app, not preview.
	‚Ä¢	Move secrets to a managed store + rotation plan (even if manual for now).

	8.	Data model trims.
	‚Ä¢	Partition project_build_events by month and add retention (e.g., 30‚Äì60 days ‚Äúverbose,‚Äù then aggregate).
	‚Ä¢	Convert build_id to uuid v7 or keep ULID but index it btree + created_at DESC for feed queries.
	‚Ä¢	Add a unique constraint on (project_id, build_seq) to make replays safe.

	9.	Tests that matter (keep it thin, but real).
	‚Ä¢	Contract tests: Next ‚Üî Worker for create/update/stream events.
	‚Ä¢	Golden event sequence test: simulate build ‚Üí assert UI states via Playwright.
	‚Ä¢	AI debit test: reserve, run, finalize, rollback on failure (and idempotency replay).

	10.	De-scope AI routing complexity.

	‚Ä¢	MVP policy: two tiers only (Premium, Standard) + a hard fallback.
	‚Ä¢	Route by explicit ‚Äúimportance‚Äù flag from the caller, not runtime ‚Äúcomplexity analysis.‚Äù
	‚Ä¢	Log token usage & cost per provider; add a daily cost cap.

Things I‚Äôd postpone (save calendar time + cognitive load)
	‚Ä¢	API gateway, service mesh, multi-region, sharding.
	‚Ä¢	Collaborative editing, version branching, A/B.
	‚Ä¢	Service worker perf tricks, aggressive prefetching, and deep translation splitting (nice later; not needed now).

Concrete acceptance checks (MVP ‚Äúdone‚Äù line)
	‚Ä¢	Sign-up ‚Üí credit check ‚Üí build ‚Üí preview ‚Üí update ‚Üí publish all green both on happy path and under a forced Worker 500.
	‚Ä¢	UI receives no state from polling when SSE is available; switching tabs doesn‚Äôt duplicate streams.
	‚Ä¢	At most one active build per project unless explicitly allowed; duplicate clicks are idempotent no-ops.
	‚Ä¢	Costs recorded within ¬±2% of provider invoices.
	‚Ä¢	Preview is fully isolated (cannot read app cookies / localStorage).
	‚Ä¢	Hook violation ESLint rule: 0 warnings.

Targeted nitpicks on the doc
	‚Ä¢	‚ÄúDual Preview Architecture‚Äù ‚Äî pick one.
	‚Ä¢	‚ÄúTierRouter with Domain Classification‚Äù ‚Äî premature for MVP; use call-site level labels.
	‚Ä¢	‚ÄúMonitoring & Observability‚Äù is listed medium-term; move it to immediate (lightweight, but essential).
	‚Ä¢	‚ÄúWebSockets‚Äù in Immediate ‚Äî prefer SSE first for lower infra & auth complexity.
	‚Ä¢	‚ÄúPerformance Optimization‚Äù (service worker, prefetch) ‚Äî only after we have traces that show where it helps.
	‚Ä¢	‚ÄúEvent Storage‚Äù ‚Äî add retention + partitions now to avoid runaway disk + slow queries later.

Quick wins (1‚Äì2 days)
	‚Ä¢	Add Idempotency-Key support in Worker and client.
	‚Ä¢	Ship a minimal SSE stream; keep existing polling as a fallback flag.
	‚Ä¢	Introduce x-request-id and propagate it everywhere (logs, errors, UI).
	‚Ä¢	Enforce ESLint react-hooks/rules-of-hooks and fix top 5 offenders.
	‚Ä¢	Add CSP + iframe sandbox on preview domain.
















Useful files I have reviewed below:
	1.	src/services/worker-api-client.ts
	2.	src/store/build-state-store.ts and useCleanBuildEvents hook
	3.	Event producer code in the Worker (the part that emits progress)
	4.	DB migrations for projects and project_build_events (and any triggers)
	5.	Preview deployment path (the endpoint + any domain config)
	6.	Your current SSE/WebSocket experiment if any (even a WIP branch)
	7.	The ‚Äúcredits‚Äù/AI time accounting code (reserve/commit/release)
	8.  src/utils/worker-auth.ts
	9.  /src/app/api/builds/[buildId]/events/route.ts


You‚Äôve got good bones, but there are a few ‚Äúmust fix‚Äùs for MVP stability + security, and a couple of simplifications that will kill a lot of the polling/registry complexity you‚Äôve built.

Below are tight, actionable recommendations grouped by area, with small code snippets you can paste in.

‚∏ª

1) Worker API client ‚Äî server-only, safer retries, consistent IDs

üî• Must-fix
	‚Ä¢	Do not sign requests in the browser. Today WorkerAPIClient can run client-side (it imports env, and generateWorkerSignature will necessarily require a secret). Make this module server-only.
	‚Ä¢	Always send a request/correlation ID, even for GETs, and reuse it across retries.
	‚Ä¢	Retry-After logic: if the server sends Retry-After seconds, don‚Äôt exponentiate it. Use it as-is. Only exponentiate when you‚Äôre guessing.
	‚Ä¢	Add timeouts to fetch and abort on exponential backoff ceiling.

Patch sketch

// src/services/worker-api-client.ts
import 'server-only'

const isServer = typeof window === 'undefined';
if (!isServer) {
  throw new Error('WorkerAPIClient must only be imported on the server');
}

export class WorkerAPIClient {
  // ...

  private buildHeaders(base: Record<string,string>, pathWithQuery: string, body: string, requestId: string) {
    const signature = generateWorkerSignature(body, pathWithQuery);
    return {
      'Content-Type': 'application/json',
      'x-sheen-signature': signature,
      'x-request-id': requestId,          // <‚Äî include for ALL methods
      ...base,
    };
  }

  async request<T>(pathWithQuery: string, options: WorkerRequestOptions = {}): Promise<T> {
    this.validateEnvironment();

    const requestId = options.__requestId ?? `next_${crypto.randomUUID()}`; // reuse on retries
    const body = options.body ?? '';
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), options.timeoutMs ?? 15000);

    const res = await fetch(`${this.baseUrl}${pathWithQuery}`, {
      ...options,
      headers: this.buildHeaders(options.headers ?? {}, pathWithQuery, body.toString(), requestId),
      signal: controller.signal,
    }).finally(() => clearTimeout(timeout));

    if (res.status === 429) {
      const rl = parseRateLimitHeaders(res.headers);
      // If server gave an authoritative wait, use that exactly
      const wait = rl.retryAfter ?? (rl.resetAt ? Math.max(0, Math.ceil((rl.resetAt.getTime() - Date.now())/1000)) : undefined);
      await this.exponentialBackoff(wait ?? 1, options.__retryAttempt ?? 0, /*authoritative=*/Boolean(wait));
      return this.request(pathWithQuery, { ...options, __retryAttempt: (options.__retryAttempt ?? 0) + 1, __requestId: requestId });
    }

    if (!res.ok) {
      await this.handleError(res, pathWithQuery);
    }

    const ct = res.headers.get('content-type') || '';
    return ct.includes('application/json') ? res.json() : (res as unknown as T);
  }

  private async exponentialBackoff(baseSeconds: number, attempt = 0, authoritative = false) {
    // If server told us how long, don‚Äôt exponentiate
    const seconds = authoritative ? baseSeconds : Math.min(baseSeconds * Math.pow(2, attempt), 60);
    const jitter = 0.1 * Math.random();
    const ms = Math.min((seconds * (1 + jitter)) * 1000, 300000);
    logger.info(`‚è≥ Retrying in ${Math.round(ms/1000)}s (attempt ${attempt+1})`);
    await new Promise(r => setTimeout(r, ms));
  }
}

Also:
	‚Ä¢	Consider a responseType option (json | binary) so you don‚Äôt cast Response to T.
	‚Ä¢	Add timestamp/nonce to the signature (x-sheen-timestamp, x-sheen-nonce) to prevent replays. (Server must verify within skew.)

‚∏ª

2) Events: stop fighting React with registries ‚Äî ship SSE and delete 70% of this hook

Your use-clean-build-events is doing Olympic gymnastics to prevent duplicate polling. SSE makes all of that unnecessary.

Server (Worker) ‚Äî add SSE route (uses your existing bus)

// worker: sseRoutes.ts
import { getCleanEventsSince } from './eventService';

app.get('/v1/builds/:buildId/events/stream', async (req, reply) => {
  const { buildId } = req.params as { buildId: string };
  const userId = req.headers['x-user-id'] as string | undefined;
  const lastId = Number(req.headers['last-event-id'] ?? 0);

  reply.raw.setHeader('Content-Type', 'text/event-stream');
  reply.raw.setHeader('Cache-Control', 'no-cache');
  reply.raw.setHeader('Connection', 'keep-alive');

  // 1) Catch-up
  const past = await getCleanEventsSince(buildId, lastId, userId);
  for (const ev of past) reply.raw.write(`id: ${ev.id}\nevent: message\ndata: ${JSON.stringify(ev)}\n\n`);

  // 2) Live
  const unsub = subscribeToEvents(buildId, (ev) => {
    // Optionally filter by userId if needed
    reply.raw.write(`id: ${ev.id}\nevent: message\ndata: ${JSON.stringify(ev)}\n\n`);
  });

  // 3) Keepalive
  const ka = setInterval(() => reply.raw.write(`event: ping\ndata: {}\n\n`), 15000);

  req.raw.on('close', () => { clearInterval(ka); unsub(); });
});

Client ‚Äî a tiny hook (no registry, no buildQueryManager)

// src/hooks/useBuildEventsSSE.ts
'use client'
import { useEffect, useRef, useState } from 'react';

export function useBuildEventsSSE(buildId: string | null, userId: string) {
  const [events, setEvents] = useState<any[]>([]);
  const [done, setDone] = useState(false);
  const lastIdRef = useRef<number>(0);

  useEffect(() => {
    if (!buildId || !userId) return;
    const es = new EventSource(`/api/proxy/worker/v1/builds/${buildId}/events/stream`, { withCredentials: false });

    es.onmessage = (msg) => {
      const ev = JSON.parse(msg.data);
      lastIdRef.current = Number(msg.lastEventId || ev.id || lastIdRef.current);
      setEvents(prev => [...prev, ev]);
      if (ev.finished) setDone(true);
    };
    es.onerror = () => { /* optional backoff + reconnect with Last-Event-ID */ };

    return () => es.close();
  }, [buildId, userId]);

  const latest = events[events.length-1];
  return {
    events,
    isComplete: Boolean(latest?.finished),
    currentProgress: latest?.overall_progress ?? 0,
    previewUrl: events.slice().reverse().find(e => e.preview_url)?.preview_url ?? null,
    phase: latest?.phase,
  };
}

You can keep React Query if you love it, but you won‚Äôt need the registry/singleton/cancellation maze. The ‚Äúone-stream-per-build‚Äù SSE connection is the singleton.

‚∏ª

3) Preview isolation ‚Äî use src, drop allow-same-origin

For real previews use a URL (your Pages preview) and make it cookieless.
	‚Ä¢	Set src={previewUrl} not doc.write.
	‚Ä¢	Sandbox should not include allow-same-origin; you want a unique opaque origin to prevent any cookie/localStorage bleed.

<iframe
  src={previewUrl ?? 'about:blank'}
  sandbox="allow-scripts allow-forms"  // üëà drop allow-same-origin
  referrerPolicy="no-referrer"
  className="w-full h-full"
/>

On the preview origin, set a tight CSP and don‚Äôt set parent-domain cookies.

‚∏ª

4) Event table ‚Äî indexes + retention now

Your common queries:
	‚Ä¢	getCleanEventsSince(buildId, lastId[, userId])

Add these:

CREATE INDEX IF NOT EXISTS idx_pbe_build_id_id
  ON public.project_build_events (build_id, id);

CREATE INDEX IF NOT EXISTS idx_pbe_build_user_id
  ON public.project_build_events (build_id, user_id, id);

-- If you keep "clean only" path
CREATE INDEX IF NOT EXISTS idx_pbe_clean
  ON public.project_build_events (build_id, id)
  WHERE user_visible = true AND event_phase IS NOT NULL;

-- 30‚Äì60d retention job (cheap for now)
CREATE OR REPLACE FUNCTION purge_old_events() RETURNS void AS $$
BEGIN
  DELETE FROM public.project_build_events
  WHERE created_at < NOW() - INTERVAL '45 days';
END$$ LANGUAGE plpgsql;
-- run nightly via cron/pg_cron

(If you expect volume, consider monthly partitions later; not necessary for MVP.)

‚∏ª

5) Billing service ‚Äî reservation + safer UPDATE math

Good start. Two things:
	1.	Reservation: startTracking should reserve estimated seconds (write a row and decrement ‚Äúavailable‚Äù now), and endTracking should settle the delta (+/-) to avoid races when multiple builds start in parallel.
	2.	Update expression safety: Don‚Äôt rely on evaluation order inside UPDATE. Compute deltas in a CTE.

WITH bal AS (
  SELECT welcome_bonus_seconds AS welcome,
         GREATEST(0, 900 - daily_gift_used_today) AS daily,
         paid_seconds_remaining AS paid,
         subscription_seconds_remaining AS sub
  FROM user_ai_time_balance
  WHERE user_id = $1
  FOR UPDATE
),
deduct AS (
  SELECT
    LEAST($billable, welcome) AS use_welcome,
    LEAST(GREATEST($billable - LEAST($billable, welcome),0), daily) AS use_daily,
    GREATEST($billable - LEAST($billable, welcome) - LEAST(GREATEST($billable - LEAST($billable, welcome),0), daily), 0) AS need_paid_total
  FROM bal
),
paid_split AS (
  SELECT
    LEAST(need_paid_total, paid) AS use_paid_col,
    GREATEST(need_paid_total - paid, 0) AS use_sub_col
  FROM bal, deduct
)
UPDATE user_ai_time_balance u
SET welcome_bonus_seconds = u.welcome_bonus_seconds - (SELECT use_welcome FROM deduct),
    daily_gift_used_today = u.daily_gift_used_today + (SELECT use_daily FROM deduct),
    paid_seconds_remaining = GREATEST(0, u.paid_seconds_remaining - (SELECT use_paid_col FROM paid_split)),
    subscription_seconds_remaining = GREATEST(0, u.subscription_seconds_remaining - (SELECT use_sub_col FROM paid_split)),
    ...
WHERE user_id = $1;

Also, make the idempotency key include sessionId (or a monotonic counter) so rebuilds of the same buildId don‚Äôt collide:

const idempotencyKey = `${params.buildId}_${params.operationType}_${params.sessionId ?? 'v1'}`;


‚∏ª

6) Auth & signing ‚Äî add anti-replay and pin the client path
	‚Ä¢	Add headers: x-sheen-timestamp, x-sheen-nonce, and include them in your HMAC canonical string (method\npath\nsha256(body)\nnonce\ntimestamp).
	‚Ä¢	Have the Worker reject signatures older than, say, 2 minutes and seen nonces (store in a small TTL cache).
	‚Ä¢	Ensure no NEXT_PUBLIC_* secret exists. Validate that in validateWorkerAuthEnvironment().

‚∏ª

7) Builder UI component

Looks fine for demoing, but it‚Äôs not the real pipeline. Two tweaks for MVP:
	‚Ä¢	Gate it behind a feature flag or move it to a /demo route so it doesn‚Äôt get confused with the production builder.
	‚Ä¢	If you keep it, don‚Äôt set allow-same-origin on the iframe (same reason as above).

‚∏ª

8) Observability (small, immediate)
	‚Ä¢	Emit x-request-id from Next ‚Üí Worker, and have Worker echo it back in responses and include in logs/events.
	‚Ä¢	Minimal metrics (Prometheus or even just counters in logs): build_latency, queue_depth, provider_error_rate, rate_limit_retries.
	‚Ä¢	Add Sentry (server + client) with buildId, projectId, userId, x-request-id tagged.

‚∏ª

9) What you can delete after SSE lands
	‚Ä¢	HookInstanceRegistry, SharedBuildEventsStore, most of buildQueryManager logic, and the many ‚ÄúisGloballyCurrent‚Äù guards.
	‚Ä¢	The polling intervals/adaptive logic, and a ton of the defensive logs.

This will dramatically reduce moving parts and hook violations.

‚∏ª

Short ‚Äúnext PRs‚Äù plan
	1.	Lock down WorkerAPIClient (server-only, request id, retry-after fix, timeout).
	2.	Add Worker SSE endpoint + minimal Next API proxy (to avoid CORS headaches).
	3.	Replace use-clean-build-events with useBuildEventsSSE (keep old behind a flag for a week).
	4.	Preview iframe: swap to src=previewUrl, remove allow-same-origin.
	5.	DB: add event indexes + nightly retention function.
	6.	Billing: idempotency key tweak; optional reservation path.
	7.	Signature: add nonce + timestamp headers and verification.

















Awesome, thanks for the real files. Here‚Äôs a focused, ‚Äúchange-this-now‚Äù pass with drop-in patches.

‚∏ª

1) worker-auth.ts ‚Äî never expose secrets + real anti-replay

Problems
	‚Ä¢	You allow NEXT_PUBLIC_WORKER_SHARED_SECRET üò¨ (that means the secret can ship to the browser).
	‚Ä¢	Signature = body + pathWithQuery only. No method, no timestamp, no nonce, no body hash ‚Üí easy replay/rehash.
	‚Ä¢	Query order differences break signatures (you don‚Äôt canonicalize/sort).

Fix (server-only, canonical, timestamped, nonce‚Äôd)
	‚Ä¢	Remove all NEXT_PUBLIC_* secret support.
	‚Ä¢	Canonicalize as:
method\npath\nsortedQuery\nsha256(body)\nnonce\ntimestamp
	‚Ä¢	Add/verify x-sheen-timestamp (¬±120s skew), x-sheen-nonce (reject replays; 5‚Äì10 min TTL set on the Worker).

// src/utils/worker-auth.ts
import 'server-only'
import crypto from 'crypto';

function sha256Hex(input: string) {
  return crypto.createHash('sha256').update(input, 'utf8').digest('hex');
}

function sortQuery(qs: string) {
  if (!qs) return '';
  const params = new URLSearchParams(qs.startsWith('?') ? qs.slice(1) : qs);
  const pairs = Array.from(params.entries()).sort(([a],[b]) => a.localeCompare(b));
  return pairs.map(([k,v]) => `${encodeURIComponent(k)}=${encodeURIComponent(v)}`).join('&');
}

export function generateWorkerSignature({
  method,
  path,           // "/v1/projects/123/export"
  query,          // "?a=1&b=2" or ""
  body,           // raw string, '' for GET
  timestamp,      // string seconds since epoch
  nonce,          // random uuid
}: {
  method: string; path: string; query: string; body: string; timestamp: string; nonce: string;
}): string {
  const secret = process.env.WORKER_SHARED_SECRET;
  if (!secret || secret.length < 32) {
    throw new Error('WORKER_SHARED_SECRET missing/too short');
  }
  const canonical =
    `${method.toUpperCase()}\n${path}\n${sortQuery(query)}\n${sha256Hex(body || '')}\n${nonce}\n${timestamp}`;
  return crypto.createHmac('sha256', secret).update(canonical, 'utf8').digest('hex');
}

export function createWorkerAuthHeaders(
  url: string, method: string, body: string, extra: Record<string,string>={}
) {
  const u = new URL(url, 'http://placeholder');
  const timestamp = Math.floor(Date.now()/1000).toString();
  const nonce = crypto.randomUUID();
  const sig = generateWorkerSignature({ method, path: u.pathname, query: u.search, body, timestamp, nonce });
  return {
    'Content-Type': 'application/json',
    'x-sheen-signature': sig,
    'x-sheen-timestamp': timestamp,
    'x-sheen-nonce': nonce,
    ...extra
  };
}

Worker-side verify sketch (Fastify on the Worker)

// check within ¬±120s, and nonce not seen before
// canonicalize exactly the same way; store nonce in a TTL cache (e.g., LRU with 10 min TTL)

Rate-limit header parser tweak (support HTTP-date Retry-After)

export function parseRateLimitHeaders(headers: Headers) {
  const out: {limit?:number;remaining?:number;resetAt?:Date;retryAfter?:number} = {};
  const parseIntSafe = (v: string | null) => (v ? parseInt(v, 10) : undefined);

  out.limit = parseIntSafe(headers.get('x-ratelimit-limit') || headers.get('ratelimit-limit'));
  out.remaining = parseIntSafe(headers.get('x-ratelimit-remaining') || headers.get('ratelimit-remaining'));

  const reset = headers.get('x-ratelimit-reset') || headers.get('ratelimit-reset');
  if (reset) {
    const n = parseInt(reset, 10);
    out.resetAt = new Date((n > 1e9 ? n*1000 : Date.now() + n*1000));
  }

  const ra = headers.get('retry-after');
  if (ra) {
    const n = parseInt(ra, 10);
    out.retryAfter = Number.isFinite(n) ? n : Math.max(0, Math.ceil((new Date(ra).getTime() - Date.now())/1000));
  }

  return out;
}


‚∏ª

2) WorkerAPIClient follow-up (ties into the auth changes)
	‚Ä¢	Mark it server-only and remove any NEXT_PUBLIC_* usage.
	‚Ä¢	Include x-sheen-timestamp, x-sheen-nonce, x-request-id on every request.
	‚Ä¢	If Retry-After is present, use it verbatim (don‚Äôt exponentiate). Exponentiate only when you‚Äôre guessing.

(If you want, I‚Äôll send a compact diff once you paste your createWorkerAuthHeaders call sites.)

‚∏ª

3) /api/builds/[buildId]/events/route.ts ‚Äî use session, not userId param, or move to SSE

Problems
	‚Ä¢	You trust userId from the querystring. A user can ask for someone else‚Äôs events.
	‚Ä¢	You use an admin Supabase client (bypasses RLS) then try to ‚Äúfilter‚Äù manually.

Quick fix (keep polling for now)
	‚Ä¢	Derive userId from the session, not from the URL.
	‚Ä¢	Call your SECURITY DEFINER function (get_user_build_events) or query with the non-admin client + RLS, filtering by both build_id and user_id = auth.uid().

// src/app/api/builds/[buildId]/events/route.ts
import { NextRequest, NextResponse } from 'next/server'
import { createServerSupabaseClient } from '@/lib/supabase/server' // your server-side helper
export const dynamic = 'force-dynamic'

export async function GET(req: NextRequest, { params }: { params: { buildId: string } }) {
  const supabase = createServerSupabaseClient();
  const { data: { user }, error: userErr } = await supabase.auth.getUser();
  if (userErr || !user) {
    return NextResponse.json({ success:false, error:'Unauthorized', events:[], lastEventId:0 }, { status: 401 });
  }

  const { buildId } = params;
  const lastEventId = Number(new URL(req.url).searchParams.get('lastEventId') ?? '0');

  // Use RLS-enforced table or your SECURITY DEFINER function
  const { data, error } = await supabase
    .from('project_build_events')
    .select('id, build_id, event_type, event_data, created_at, user_id, event_phase, event_title, event_description, overall_progress, finished, preview_url, error_message, duration_seconds')
    .eq('build_id', buildId)
    .eq('user_id', user.id)
    .eq('user_visible', true)
    .gt('id', lastEventId)
    .order('id', { ascending: true })
    .limit(100);

  if (error) return NextResponse.json({ success:false, error:'DB error', events:[], lastEventId:0 }, { status: 500 });

  // ‚Ä¶transform to CleanBuildEvent exactly as you do now‚Ä¶
  // compute newLastEventId from returned rows
  return NextResponse.json({ buildId, events, lastEventId: newLastEventId });
}

Better fix (what I recommend): switch this endpoint to SSE and proxy to the Worker**
	‚Ä¢	Next route: /api/builds/[buildId]/events/stream
	‚Ä¢	Authenticates the user (session)
	‚Ä¢	Opens a streaming connection to the Worker‚Äôs SSE (or hits the DB and emits SSE itself), filtering by the session user server-side
	‚Ä¢	Returns text/event-stream
	‚Ä¢	Client: replace the poller with a tiny EventSource hook.

If you want, I‚Äôll drop a minimal Next SSE proxy that bridges the Worker‚Äôs SSE (it‚Äôs ~30 lines).

‚∏ª

4) Worker event bus ‚Üí make per-user filtering explicit for SSE

Your bus.emit(buildId, userEvent) is good, but the bus listener has no built-in user filtering. When you build SSE:
	‚Ä¢	Either emit on a composite channel: bus.emit(${buildId}:${userId}, ‚Ä¶) and subscribe to that exact channel, or
	‚Ä¢	Subscribe to buildId and filter in the SSE route by userId === sessionUser.

(Do not trust a userId from the client.)

‚∏ª

5) DB: add the indexes you‚Äôll need (cheap, high win)

CREATE INDEX IF NOT EXISTS idx_pbe_build_id_id
  ON public.project_build_events (build_id, id);

CREATE INDEX IF NOT EXISTS idx_pbe_build_user_id
  ON public.project_build_events (build_id, user_id, id);

CREATE INDEX IF NOT EXISTS idx_pbe_clean_only
  ON public.project_build_events (build_id, id)
  WHERE user_visible = true AND event_phase IS NOT NULL;

-- Optional retention (run nightly)
DELETE FROM public.project_build_events WHERE created_at < NOW() - INTERVAL '45 days';


‚∏ª

6) Preview iframe: lock it down

In your builder component, change:

<iframe
  src={previewUrl ?? 'about:blank'}
  sandbox="allow-scripts allow-forms"   // drop allow-same-origin
  referrerPolicy="no-referrer"
  className="w-full h-full"
/>

And keep the preview on a cookieless subdomain.

‚∏ª

7) Billing: idempotency key & reservation (small tweaks)
	‚Ä¢	Make the idempotency key include session/run granularity:

const idempotencyKey = `${params.buildId}_${params.operationType}_${params.sessionId ?? 'v1'}`;

	‚Ä¢	(Next step) add a simple reservation at startTracking (write a row + provisional decrement), and reconcile at endTracking. That prevents double-spend under concurrent starts.

‚∏ª

What I can do next for you
	‚Ä¢	Send a PR-ready patch for: server-only Worker client + new HMAC headers + Next SSE route.
	‚Ä¢	A tiny Fastify verify middleware for the Worker (x-sheen-timestamp/nonce validation + TTL cache).
	‚Ä¢	The EventSource React hook to replace your polling/registry code.

If you paste:
	‚Ä¢	your Worker Fastify auth/verify middleware (or where you want it),
	‚Ä¢	your Next server Supabase helper (createServerSupabaseClient),
I‚Äôll tailor exact diffs.
















Question to the expert: Currently any messages sent over the builder chat are not saved anywhere (as far as i know). Given your understanding now of our platform and how the users and our app use the chat, what do you recommend?

Answer:
Short version: you should treat chat as a first-class, auditable interface to the builder‚Äînot just UX chrome. Save it. Tie it to projects/builds. Make it stream. Keep costs + privacy in mind.

Here‚Äôs the lean plan that pays off immediately and won‚Äôt bog the MVP.

Why save chat (in this product)
	‚Ä¢	Repro + support: ‚ÄúWhy did my site look like this?‚Äù ‚Üí load the thread and the exact build it triggered.
	‚Ä¢	Context carryover: Keep state across sessions; let users resume.
	‚Ä¢	Product signals: Which prompts/features are used, cost per convo, where users get stuck.
	‚Ä¢	Safety: Abuse reports / GDPR export & delete.

Minimal data model (Postgres / Supabase)

Keep it boring and future-proof. Three tables + one view:

-- A conversation anchored to a project/version
create table chat_threads (
  id uuid primary key default gen_random_uuid(),
  project_id uuid not null,
  user_id uuid not null,
  title text,
  locale text default 'en',
  last_message_at timestamptz default now(),
  created_at timestamptz default now(),
  archived boolean default false
);

-- Messages with rich payloads, tool calls & links to builds
create type chat_role as enum ('user','assistant','system','tool');
create table chat_messages (
  id bigserial primary key,
  thread_id uuid not null references chat_threads(id) on delete cascade,
  role chat_role not null,
  content jsonb not null,        -- {text, parts, chips, attachments, ‚Ä¶}
  model text,                    -- e.g. gpt-4o, claude-3.5
  provider text,
  tokens_input int,
  tokens_output int,
  cost_usd numeric(10,5),
  build_id text,                 -- if this message triggered a build
  build_event_id int,            -- optional pointer to first event
  created_at timestamptz default now()
);

-- One row per model call (ties into your billing)
create table chat_runs (
  id bigserial primary key,
  thread_id uuid not null references chat_threads(id) on delete cascade,
  user_id uuid not null,
  request jsonb not null,        -- prompt+tools (redacted)
  response jsonb not null,       -- completion+tool_calls (redacted)
  latency_ms int,
  tokens_input int,
  tokens_output int,
  cost_usd numeric(10,5),
  success boolean default true,
  created_at timestamptz default now()
);

Indexes & RLS

create index on chat_messages (thread_id, id);
create index on chat_threads (user_id, project_id, last_message_at desc);
-- RLS: user can only see own threads/messages; service role full access.

Write/stream path (simple & robust)
	1.	Client ‚Üí Next (server action/API): POST /api/projects/:projectId/chat/:threadId/messages
	‚Ä¢	Create chat_messages(role='user')
	‚Ä¢	Optionally enqueue a Worker task (or call Worker sync) to process with AI.
	2.	Worker processes AI call
	‚Ä¢	Record chat_runs (prompt/response/tokens/latency/cost).
	‚Ä¢	Emit assistant message rows incrementally (stream tokens via SSE/WebSocket if you want live typing), then a final chat_messages(role='assistant').
	‚Ä¢	If it triggers a build, set build_id on the assistant message and start your existing build event flow.
	3.	Frontend subscribes
	‚Ä¢	Use SSE channel per thread (/v1/chat/threads/:threadId/stream) ‚Üí minimal code, matches your planned build SSE.

Message shape (JSON you can evolve)

{
  "text": "Add a pricing page with 3 tiers",
  "chips": ["3 tiers","Add FAQ"],            // for your current UI
  "attachments": [{"type":"image","url":"..."}],
  "metadata": {
    "locale": "en",
    "ui_origin": "builder_left_panel",
    "versionId": "v_123",
    "previewUrl": "https://‚Ä¶"
  }
}

Assistant messages can include:

{
  "text": "Done. Deployed preview here:",
  "actions": [{"type":"open_preview","url":"https://‚Ä¶"}],
  "diff_summary": {"files_changed": 7},
  "triggeredBuild": {"buildId":"01HV‚Ä¶","phase":"deploy"}
}

Tie-ins with your existing systems
	‚Ä¢	Builds: store build_id on the assistant message that initiated it; link from the build completion event back to chat_messages.id (handy in the UI).
	‚Ä¢	Billing: add a new operationType = 'chat' path to your aiTimeBillingService so chat usage is accounted separately from main_build/update. Use the same idempotency approach.
	‚Ä¢	Internationalization: save locale per thread; you can train routing (e.g., Arabic copy generation) later.
	‚Ä¢	Search: later, add an embeddings table for semantic search across a user‚Äôs threads. Not needed for MVP.

Privacy, safety, retention (don‚Äôt skip)
	‚Ä¢	PII redaction on server before writing request/response into chat_runs (keep the full chat_messages for product value, but consider a ‚ÄúDo not save content‚Äù toggle per thread for enterprise).
	‚Ä¢	Retention: keep raw chat_runs.request/response for 30‚Äì60 days; keep chat_messages indefinitely (or until project is deleted). Add a nightly job to trim runs.
	‚Ä¢	Export & delete: one endpoint can dump a thread (messages + builds) for GDPR requests. Easy because it‚Äôs all relational.

Small UI changes
	‚Ä¢	In builder-interface.tsx, persist on every send:
	‚Ä¢	POST /api/projects/:projectId/chat/{threadId}/messages with {role:'user', content:{text, chips‚Ä¶}}
	‚Ä¢	Show assistant replies from the stream (don‚Äôt wait for full completion).
	‚Ä¢	When a chip is clicked, save it as a user message too (not just UI state).

Quick implementation steps (1 week-ish)
	1.	Create the 3 tables + RLS + indexes.
	2.	Add POST /api/chat and GET /api/chat/:threadId/messages?after=‚Ä¶ (or straight SSE).
	3.	Worker: on reply, write chat_runs and a final chat_messages(role='assistant'); stream via SSE.
	4.	Frontend: switch to thread-driven state (threadId in URL), and render history from DB rather than in-memory arrays.
	5.	Add operationType='chat' to billing; show token/cost in a collapsible message footer (optional but nice).

If you want, send me your current server route that calls the AI (or where that logic lives), and I‚Äôll wire in the DB writes + a tiny SSE stream so the chat feels live and everything‚Äôs stored correctly.


Question to the expert:
Ok but remember that our ui promises two modes of chat: build and plan. The build mode takes the user input and sends it to the worker (where is gets processed and augmented) to be treated as a prompt for action and instructions. The plan mode is simply to ask questions, talk over ideas, make plans before executing them in the build mode,... etc. I think it would also be smart to plan a smart, decent and user friendly way for our users to move between either modes. What do you think?


Answer:
Totally. Treat Plan and Build as two modes of the same conversation‚Äînot two separate features. Users should brainstorm, then promote ideas into actions with one click. Here‚Äôs a tight plan you can ship without derailing the MVP.

Product shape (how it should feel)
	‚Ä¢	One thread per project, two modes in the composer: a segmented control: Plan | Build.
	‚Ä¢	Plan (default): free Q&A, ideation, content help, ‚Äúwhat if‚Ä¶‚Äù. Zero side effects. Cheaper models.
	‚Ä¢	Build: executes changes; kicks the Worker; shows progress and preview.
	‚Ä¢	Inline ‚ÄúMake it‚Äù: every Plan message the model flags as actionable gets a small CTA (and keyboard shortcut) to convert it into a draft build instruction. Users can edit ‚Üí confirm ‚Üí run.
	‚Ä¢	Review drawer: when you switch from Plan ‚Üí Build, open a drawer that proposes a concise Build Instruction computed from the recent plan discussion. One big ‚ÄúRun build‚Äù button.
	‚Ä¢	Safety affordances:
	‚Ä¢	Plan mode never performs actions.
	‚Ä¢	Build always shows a confirmation card: credits estimate, diff summary (if available), and preview target.
	‚Ä¢	Destructive actions (delete/reset) = explicit warning.

Data model (minimal, future-proof)

Add three columns + one new table.

-- 1) Threads already exist (see earlier), add defaults
alter table chat_threads add column if not exists default_mode text check (default_mode in ('plan','build')) default 'plan';
alter table chat_threads add column if not exists project_id uuid not null; -- (if not already)

-- 2) Messages: mark which mode produced the message + detected intent
alter table chat_messages add column if not exists mode text check (mode in ('plan','build')) default 'plan';
alter table chat_messages add column if not exists assistant_intent jsonb; -- {actionable:boolean, actions:[...], confidence:0.0-1.0}

-- 3) Draft/build instructions created from plan
create table if not exists chat_actions (
  id uuid primary key default gen_random_uuid(),
  thread_id uuid not null references chat_threads(id) on delete cascade,
  user_id uuid not null,
  source_message_id bigint,                  -- plan message that generated this (nullable)
  status text check (status in ('draft','reviewed','queued','running','completed','failed')) default 'draft',
  instruction jsonb not null,                -- normalized BuildInstruction
  build_id text,                             -- filled once queued
  created_at timestamptz default now(),
  updated_at timestamptz default now()
);

create index if not exists idx_chat_actions_thread on chat_actions(thread_id, created_at desc);
create index if not exists idx_chat_messages_thread on chat_messages(thread_id, id);

BuildInstruction (normalized payload handed to the Worker)

Keep this stable so both Plan‚ÜíBuild and direct Build use the same contract.

{
  "projectId": "uuid",
  "userId": "uuid",
  "goals": ["Add pricing page with 3 tiers"],
  "changes": [
    {"type":"add_page","slug":"pricing","sections":["hero","tiers","faq"]},
    {"type":"update_copy","path":"/home","selector":"h1","text":"Acme ‚Äî fast billing"}
  ],
  "constraints": {"brand":"Acme", "palette":"indigo", "locale":"en"},
  "notes": "Prefer minimal layout; keep CLS low"
}

API flow (simple endpoints)
	‚Ä¢	Plan message
POST /api/projects/:projectId/threads/:threadId/messages
Body: {role:'user', mode:'plan', content:{text}}
‚Üí Server calls Plan model (cheap), stores assistant reply (mode:'plan').
‚Üí Also runs intent detection to set assistant_intent and (optionally) create a chat_actions row with status='draft'.
	‚Ä¢	Convert to Build
POST /api/threads/:threadId/actions ‚Üí create/update chat_actions with status='reviewed'.
UI shows drawer with the BuildInstruction (editable JSON ‚Üí form).
Confirm ‚Üí POST /api/actions/:id/queue ‚Üí calls Worker; sets status='queued', writes build_id, adds a build-mode assistant message (e.g., ‚ÄúStarting build 01AB‚Ä¶‚Äù) and kicks your SSE build stream.
	‚Ä¢	Direct Build
If the composer is in Build mode, the server creates the same chat_actions row from the message and immediately queues it.
	‚Ä¢	Streaming
	‚Ä¢	Chat threads stream via SSE: /api/chat/threads/:threadId/stream (user-scoped).
	‚Ä¢	Build events stream via SSE you‚Äôre adding for builds.
Both streams attach to the same conversation view.

Mode switching UX (friendly + explicit)
	‚Ä¢	Composer segmented control Plan | Build with short tooltips:
	‚Ä¢	Plan: ‚ÄúAsk, explore ideas, no changes happen.‚Äù
	‚Ä¢	Build: ‚ÄúApply changes to your site (will consume credits).‚Äù
	‚Ä¢	Keyboard: ‚åò/Ctrl + B toggles mode; Enter sends; ‚åò/Ctrl + Enter ‚Üí ‚ÄúConvert last plan draft to build‚Äù.
	‚Ä¢	On first switch Plan‚ÜíBuild in a thread, show a one-time explainer and remember ‚Äúdon‚Äôt show again‚Äù.
	‚Ä¢	In Plan messages flagged actionable, show a subtle inline Make it button (primary action colored).
Clicking opens the review drawer pre-filled from the intent.

Intent detection (tiny, reliable, cheap)
	‚Ä¢	Don‚Äôt overfit an ML classifier. Use a few-shot prompt on your Standard tier model:
	‚Ä¢	Input: latest user message + last 5 messages.
	‚Ä¢	Output JSON: {actionable:boolean, actions:[‚Ä¶], confidence:0.0-1.0, instruction:BuildInstruction}.
	‚Ä¢	Save this JSON in assistant_intent; if actionable && confidence>=0.6, auto-create a chat_actions draft (but never auto-run).

Billing rules (keep it simple)
	‚Ä¢	Plan: operationType='chat' (your billing service). Use cheapest viable model.
	‚Ä¢	Build: your existing main_build/update.
	‚Ä¢	Show a pre-flight estimate in the confirmation drawer using aiTimeBillingService.estimateDuration(); if insufficient balance, surface the same structured 402 path you already have.

Safety rails
	‚Ä¢	Plan cannot mutate state. Even if the user says ‚Äúdeploy now‚Äù in Plan, it only produces a draft action.
	‚Ä¢	Build always requires an explicit confirmation (the drawer).
	‚Ä¢	For destructive inferred actions (e.g., ‚Äúdelete blog‚Äù), set instruction.requiresConfirmation=true and show an extra confirmation.

Minimal UI changes you can ship next
	1.	Add the Plan | Build segmented control to your builder chat composer.
	2.	When sending in Plan, persist messages and run the intent detector; render inline ‚ÄúMake it‚Äù.
	3.	Implement the review drawer (title: ‚ÄúReview build instruction‚Äù), with basic fields: goals, pages, constraints ‚Üí ‚ÄúRun build‚Äù.
	4.	On ‚ÄúRun build‚Äù, create chat_actions and queue the Worker job; start listening to build SSE; show progress on the right panel you already have.

Example quick endpoint (Plan send ‚Üí detect intent)

// POST /api/projects/:projectId/threads/:threadId/messages
export async function POST(req) {
  const { user } = await auth(); if (!user) return unauthorized();
  const { projectId, threadId } = req.params;
  const { text, mode } = await req.json();

  // 1) save user message
  const msgId = await db.insertMessage({ threadId, role:'user', mode, content:{text}, userId:user.id });

  if (mode === 'plan') {
    // 2) get assistant reply (cheap model)
    const reply = await planModel(text, /*context*/);
    const aiMsgId = await db.insertMessage({ threadId, role:'assistant', mode:'plan', content:{text: reply.text} });

    // 3) detect intent
    const intent = await detectIntent(text, /*history*/);
    await db.updateMessage(aiMsgId, { assistant_intent: intent });
    if (intent.actionable && intent.confidence >= 0.6) {
      await db.insertAction({ threadId, userId:user.id, source_message_id: aiMsgId, instruction: intent.instruction });
    }

    return json({ ok:true });
  }

  // mode === 'build'
  const instruction = await instructionFromSingleMessage(text, /*history*/);
  const actionId = await db.insertAction({ threadId, userId:user.id, instruction, status:'reviewed' });
  return json({ actionId }); // client will open the review drawer
}

Analytics you‚Äôll actually use
	‚Ä¢	% of plan messages flagged actionable, % converted to build, plan‚Üíbuild time, first build success rate, avg cost per plan Q, and drop-off points (drawer opened but not run).

Acceptance checks
	‚Ä¢	In Plan, nothing mutates server state.
	‚Ä¢	Any build is traceable to: thread ‚Üí message ‚Üí chat_action ‚Üí build_id.
	‚Ä¢	Users can convert a plan idea to a build in ‚â§ 2 clicks.
	‚Ä¢	If balance is insufficient, they see a friendly upsell with exact needed seconds.

‚∏ª

If you want, share your current chat send route (or where you intend to put it), and I‚Äôll wire in:
	‚Ä¢	the intent detector prompt,
	‚Ä¢	the chat_actions insert,
	‚Ä¢	the review drawer payload shape,
	‚Ä¢	and the small front-end changes to the composer (toggle + Make it).
